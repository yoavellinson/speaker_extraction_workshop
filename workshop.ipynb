{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SoRAIM winter school - Speaker Extraction Workshop \n",
    "#### Hosted by Bar Ilan University (BIU)\n",
    "prior knowledge: \n",
    "- Minor understanding of signal processing \n",
    "- Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Intruduction (10 min)\n",
    "<img src=\"media/ros4hri_ids.png\" alt=\"h\" width=\"700\"/>\n",
    "\n",
    "*https://wiki.ros.org/hri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech processing in HRI\n",
    "\n",
    "In the SPRING project the patients and ARI are having a verbal dialogue. Therefore the ability to understand the patient is key.\n",
    "When failing to understand the patient, ARI could respond in a completely wrong matter,which can damage the expectance of ARI by the patient.\n",
    "Scene the patient's speech might be overlapped with another human's voice (doctors,accompanying person,background speech etc.) we need to encounter and somehow get the wanted speech only.\n",
    "\n",
    "\n",
    "There are two main ways of doing such task:\n",
    " 1. \"Blind\" source separation - splits the overlapping speech into two channels (one for each speaker) with no prior knowledge about the speakers.\n",
    " 2. Speaker extraction - extract the desired speaker from the overlapping speech utilizing a reference audio signal thereof.\n",
    "\n",
    "Today we will focus on speaker extraction.\n",
    "## Lets see an example\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture --no-display\n",
    "# installing requierments\n",
    "! pip3 install requests ffmpeg-python\n",
    "! pip3 install pip install \"deepgram-sdk>=3.*,<4.0\"\n",
    "! pip3 install -r requirements.txt\n",
    "import gdown\n",
    "id = \"19WkOv8XQ4lQwtPQLp5TWMY_xDAcs0ArW\"\n",
    "output = 'speaker_extraction/epoch=374,val_loss=-12.62.pth'\n",
    "gdown.download(id=id,output=output)\n",
    "from workshop_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import jiwer\n",
    "mixed,sr = sf.read('example/mix.wav')\n",
    "mixed_trns = transcribe_wav('example/mix.wav')\n",
    "truth =\"Hello, robot. I would like to know where is room number 55, I have an appointment in 5 minutes, and I don't want to be late.\" \n",
    "wer_before = jiwer.wer(truth,mixed_trns)\n",
    "print(f\"Mixture transctiprion WER={wer_before}:\\n{mixed_trns}\")\n",
    "display_audio(mixed,sr)\n",
    "print('_____________________________________________')\n",
    "print('The refrence:')\n",
    "ref,sr = sf.read('example/male_ref.wav')\n",
    "display_audio(ref,sr)\n",
    "print('_____________________________________________')\n",
    "\n",
    "\n",
    "from speaker_extraction.extraction_model.deliver import Extractor\n",
    "extractor = Extractor()\n",
    "e_wav = extractor.extract_wave('example/mix.wav','example/male_ref.wav','example/')[:int(mixed.shape[0]*(8000/sr))]\n",
    "sf.write('example/extracted.wav',e_wav,8000)\n",
    "extracted_trns = transcribe_wav('example/extracted.wav')\n",
    "wer_after = jiwer.wer(truth,extracted_trns)\n",
    "print(f\"Extracted speech transctiprion, WER={wer_after}:\\n{extracted_trns}\")\n",
    "display_audio(e_wav,8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The tasks for today's workshop:\n",
    "- [ ] Basics speaker extraction.\n",
    "- [ ] Audio recording and feature extraction of audio data.\n",
    "- [ ] Extracting your own voice from a mixture of two voices.\n",
    "- [ ] Evaluating your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basics of Speaker Extraction (20 minutes)\n",
    "\n",
    "#### Target speech/speaker extraction (TSE) isolates the speech signal of a target speaker from a mixture of several speakers, with or without noises and  reverberations, using clues that identify the speaker in the mixture. Such clues might be a spatial clue indicating the direction of the target speaker, a video of the speaker’s lips, and a prerecorded enrollment utterance from which the speaker’s voice characteristics can be derived. \n",
    "<img src=\"media/delcroix01-3240008-large.gif\" alt=\"extraction_clues\" width=\"700\">\n",
    "\n",
    "- **Paragraph and image:** K. Zmolikova, M. Delcroix, T. Ochiai, K. Kinoshita, J. Cernocký, and D. Yu, ‘‘Neural target speech extraction: An overview,’’ IEEE Signal Process. Mag., vol. 0,  o. 3, pp. 8–29, May 2023 \n",
    "  \n",
    "\n",
    "#### In our approach, we propose a Siamese-Unet architecture that uses both representations. The Siamese encoders are applied in the frequency domain to infer the embedding of the noisy and reference spectra, respectively. The concatenated representations are then fed into the decoder to estimate the real and imaginary  components of the desired speaker, which are then inverse-transformed to the time-domain.\n",
    "\n",
    "- A. Eisenberg, S. Gannot and S. E. Chazan, \"Single microphone speaker extraction using unified time-frequency Siamese-Unet,\" 2022 30th European Signal Processing Conference (EUSIPCO), Belgrade, Serbia, 2022, pp. 762-766, doi: 10.23919/EUSIPCO55093.2022.9909545. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the process of recording speech samples on laptops:\n",
    "#### The audio samples in this section is from the Librispeech dataset from OpenSLR: https://www.openslr.org/11/\n",
    "Audio is represented as a list of samples that correspond to the sound and time axis. \n",
    "Normalizing audio signal can be done in several ways, we will normalize in this method: \n",
    "$$\n",
    "X_{norm} = \\frac{0.9\\cdot X }{max(abs(X))}\\\n",
    "$$\n",
    "for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "samples,sr = sf.read('audio_samples/males/28233/5105-28233-0006.wav')\n",
    "print(samples[20:])\n",
    "print(f'samplerate={sr}')\n",
    "\n",
    "# TODO: implement the following functions\n",
    "# hint when mixing two audio signals you add both signals and then normalize, make sure both signals has same length before summation!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### YOUR CODE HERE\n",
    "\n",
    "def pad_signal_to_length():\n",
    "    pass\n",
    "def norm_signal(samples):\n",
    "    pass\n",
    "\n",
    "def mix_signals(wav1,wav2,sir=0): #mixes two audio signals with sir\n",
    "    #Riminder! check lengths\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable **wav** represent the audio samples and the variable **sr** represents the sampling rate of the audio.\n",
    "\n",
    "The sampling rate corresponds to the number of samples this file has in one second.\n",
    "\n",
    "The sampling rate is important for processing and playback as well, playing the same file with a wrong sampling rate will result in distortion of the original audio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"> For the next parts audio will be played on your PC, please keep the volume down or use headphones </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import torch\n",
    "print('Audio with correct sample rate')\n",
    "def display_audio(samples,sr):\n",
    "    if torch.is_tensor(samples):\n",
    "        samples= samples.detach().numpy()\n",
    "    ipd.display(ipd.Audio(samples,rate=sr))\n",
    "display_audio(samples,sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recording audio using Audacity:\n",
    "\n",
    "1. open Audacity\n",
    "2. choose 1 input channel (mono) (A)\n",
    "3. click record (B)\n",
    "   \n",
    "<img src=\"media/audacity_1.png\" width=\"700\" >\n",
    "\n",
    "4. then export the audio using: file -> export -> export as WAV\n",
    "5. save .wav file and copy the file to your working directory of this notebook.\n",
    "6. load file using soundfile \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hands-on Session #1 : Signals and Spectograms (15 minutes)\n",
    "### 3.a. Recording Speech Samples\n",
    "record your self saying the following phrase:\n",
    "- Hello from Grenoble! Today I learned about audio processing and speech extraction, Let's see if my voice could be extracted.\n",
    "\n",
    "Export the audio to a .wav file and then record your self introducing yourself - should be a few second long.\n",
    "Export the second .wav file.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Load both files to two variables and play the files one after the other.\n",
    "#2. play one of the files with sr != sr, how it sounds?\n",
    "########## YOUR CODE HERE #######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.b. Hands-on Session: STFT domain\n",
    "\n",
    "Now that you are familiar with digital audio, lets talk about spectrograms.\n",
    "\n",
    "The sampled audio as a discrete-time signal that can be represented and processed in the time domain and in the frequency domain. \n",
    "\n",
    "Because the speech is a signal with large time variation (non-stationary signal), it is mandatory to use the STFT rather than simple fourier transform.\n",
    "\n",
    "The STFT of the audio signal will give us the DFT (complex value) for each time frame, and by taking the absolute value of the STFT we get the spectrogram of the audio.\n",
    "\n",
    "The spectrogram is used as an input feature to many deep learning algorithms. This way, the audio data can be thought of as an image. Among other processing methods, we can also adopt image processing techniques to analyze the audio signal and process it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import numpy as np\n",
    "\n",
    "def plot_spectrogram(samples,sr,nfft=512):\n",
    "    samples = samples.squeeze()\n",
    "    hop_frac=4\n",
    "    if not torch.is_tensor(samples):\n",
    "        samples = torch.tensor(samples)\n",
    "    samples_stft = torch.stft(samples,\n",
    "                        n_fft=nfft,\n",
    "                        hop_length=nfft//hop_frac,\n",
    "                        win_length=nfft,\n",
    "                        window=torch.hamming_window(nfft),\n",
    "                        center=True,\n",
    "                        pad_mode='reflect',\n",
    "                        normalized=False,\n",
    "                        onesided=None,\n",
    "                        return_complex=True)\n",
    "    spec = torch.abs(samples_stft)\n",
    "    plt.figure(figsize = (10,10))\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Frequency [Hz]')\n",
    "    plt.yticks(np.arange(0,(nfft//2)+1 ,16),(np.arange(0,(nfft//2)+1 ,16)*(sr/(2*nfft//2))).astype(np.int16))\n",
    "    plt.xticks(np.arange(0,spec.shape[-1],50),np.arange(0,spec.shape[-1],50)*nfft/(hop_frac*sr))\n",
    "    ax = plt.gca()\n",
    "    spec_plot =ax.imshow(20*torch.log10(spec),origin='lower')\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    cbar = plt.colorbar(spec_plot,cax=cax)\n",
    "    cbar.set_label('Energy (dB)')\n",
    "    plt.tight_layout()\n",
    "    ax.set_title('Spectogram of the audio data in dB')\n",
    "    plt.show()\n",
    "\n",
    "s,sr = sf.read('speaker_extraction/outputs/mono_s1/ref1.wav')\n",
    "plot_spectrogram(s,sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. use this function to plot your Speech spectrogram\n",
    "#2. play with the parameters: nfft, hop and win_length  - what is the efect on the spectrogram?\n",
    "\n",
    "#### YOUR CODE HERE #######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Extraction model\n",
    "The model arcitecture:\n",
    "\n",
    "<img src=\"media/extraction_model.png\" width=\"700\">\n",
    "\n",
    "Our model works with audio sampled at 8000 Hz, it will automaticly resample the audio will also return the extraacted audio in 8000 Hz.\n",
    "When playing the extracted audio make sure to set SR=8000 !\n",
    "\n",
    "TDL:\n",
    "- [] load two audio files with **torchaudio** (HINT 1), one to be extracted and another one as interference (could be another workshop memeber or one from samples folder)\n",
    "- [] Mix two files (make sure to have two diffrent speakers)\n",
    "- [] Load a refrence file with torchaudio as well\n",
    "- [] Start a speaker extration class instance (HINT 2)\n",
    "- [] Use the extract_vec method to extract your voice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because working with pytorch we can load the audio with torchaudio directly to a torch.Tensor\n",
    "import torchaudio\n",
    "#HINT 1\n",
    "#samples_mix, sr = torchaudio.load(path_to_wav)\n",
    "\n",
    "#HINT 2: to use the model import it with the commands:\n",
    "# from speaker_extraction.extraction_model.deliver import Extractor\n",
    "# extractor = Extractor()\n",
    "\n",
    "# Plot the spectrogram of the mixed signal and then the extracted signal, Do you see any difference?\n",
    "\n",
    "####### YOUR CODE HERE #######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation and Discussion (10 minutes)\n",
    "The Si-SDR (Scale invariant signal-to-distortion ratio) is a widely used metric in the field of audio signal processing, particularly in the context of source separation. \n",
    "$$\n",
    "\\text{Si-SDR} = 10 \\log_{10} \\left( \\frac{{\\| \\frac{<\\hat{s},s>}{<s,s>}\\ s\\|^2}}{{\\| \\frac{<\\hat{s},s>}{<s,s>}\\ s -\\hat{s} \\|^2}} \\right)\n",
    "\n",
    "$$\n",
    "The Si-SDR is used measure the quality of separation between the original signal and the extracted signal with adaptation to the gain that the model might add to the extracted signal. \n",
    "We use the Soi-SDRi (improved) which is the deference between the Si-SDR of the output and the mixture, and the Si-SDR of the output and the real audio of the speaker (label).\n",
    "\n",
    "The Si-SDR is suseptable to time shift and destortion, but is not sesative to changes in gain.\n",
    "Larger Si-SDR denotes a better extraced sentence in this context.\n",
    "\n",
    "# WER \n",
    "Word Error Rate (WER) is a common metric of the performance of a speech recognition or machine translation system. A lower WER denotes a better transciption of the audio.\n",
    "$$\n",
    "WER = \\frac{S+I+D}{N}\n",
    "$$\n",
    "Where N is the number of words in the sentence,\n",
    "\n",
    "I is the number of insertions,\n",
    "\n",
    "D is the number of deletions, \n",
    "\n",
    "S is the number of substitutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.audio import SignalDistortionRatio\n",
    "from speaker_extraction.extraction_model.deliver import Extractor\n",
    "import torchaudio.functional as F\n",
    "from workshop_utils import *\n",
    "extractor = Extractor()\n",
    "si_sdr = SignalDistortionRatio()\n",
    "\n",
    "def prepare_batch(wav1_path,wav2_path,ref1_path,ref2_path,input_sr =16000):\n",
    "    #returns mixed signal, ref1, ref2, org wav1 resampled to 8000, org wav2 resampled to 8000\n",
    "    wav1,sr_wav1 = torchaudio.load(wav1_path)\n",
    "    wav2,sr_wav2 = torchaudio.load(wav2_path)\n",
    "    ref1,sr_ref1 = torchaudio.load(ref1_path)\n",
    "    ref2,sr_ref2 = torchaudio.load(ref2_path)\n",
    "    if (sr_ref1 + sr_ref2 +sr_wav1+sr_wav2)/4 != input_sr:\n",
    "        raise(Exception('input sr does not match'))\n",
    "    mixed = mix(wav1,wav2)\n",
    "    wav1_res = F.resample(wav1,sr_wav1,8000)\n",
    "    wav2_res = F.resample(wav2,sr_wav2,8000)\n",
    "    mixed_res = F.resample(mixed,input_sr,8000)\n",
    "    max_len = max(mixed_res.shape[-1],wav1_res.shape[-1],wav2_res.shape[-1])\n",
    "    mixed_res = pad_to_length(mixed_res,max_len)\n",
    "    wav1_res = pad_to_length(wav1_res,max_len)\n",
    "    wav2_res = pad_to_length(wav2_res,max_len)\n",
    "    return mixed,mixed_res,ref1,ref2,wav1_res,wav2_res\n",
    "\n",
    "wav1_path='audio_samples/males/28233/5105-28233-0006.wav'\n",
    "wav2_path='audio_samples/females/29095/4970-29095-0034.wav'\n",
    "ref1_path='audio_samples/males/28233/5105-28233-0008.wav'\n",
    "ref2_path='audio_samples/females/29095/4970-29095-0038.wav'\n",
    "\n",
    "input_sr=16000\n",
    "mixed,mixed_resampled,ref1,ref2,y_1,y_2 = prepare_batch(wav1_path,wav2_path,ref1_path,ref2_path,input_sr)\n",
    "plot_spectrogram(mixed,16000)\n",
    "sf.write('outputs/mixed_f1_m1.wav',mixed_resampled,8000)\n",
    "print(f\"Mixed signal:\\n{transcribe_wav('outputs/mixed_f1_m1.wav')}\")\n",
    "display_audio(mixed,sr)\n",
    "\n",
    "y_hat = extractor.extract_vec(mix =mixed.unsqueeze(0),sr_mix=16000,ref=ref1,sr_ref=16000)\n",
    "# y_hat = pad_to_length(y_hat,mixed_resampled.shape[-1])\n",
    "plot_spectrogram(y_hat,8000)\n",
    "display_audio(y_hat,8000)\n",
    "sf.write('outputs/y_hat_f1_m1.wav',y_hat,8000)\n",
    "print(f\"Extracted Signal:\\n{transcribe_wav('outputs/y_hat_f1_m1.wav')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def si_sdri(y,y_hat,mixed):\n",
    "    y = y.squeeze()\n",
    "    y_hat = y_hat.squeeze()\n",
    "    mixed=  mixed.squeeze()\n",
    "    input_sisdr =si_sdr(y_hat,mixed_resampled)\n",
    "    output_sisdr = si_sdr(y_hat,y_1)\n",
    "    return output_sisdr - input_sisdr\n",
    "\n",
    "print(si_sdri(y_1,y_hat,mixed_resampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion (10 minutes)\n",
    "## Anyone is keen to share his/hers results?\n",
    "\n",
    "- We saw how to improve ASR dramatically with speaker extraction, when the wanted speaker is known to the robot.\n",
    "  \n",
    "- Additional resources for further learning. (papers, Sharon's website)\n",
    "  1. Sharons website: https://sharongannot.group/\n",
    "  2. papers: \n",
    "    - A. Eisenberg, S. Gannot and S. E. Chazan, \"A two-stage speaker extraction algorithm under adverse acoustic conditions using a single-microphone\" : https://arxiv.org/abs/2303.07072\n",
    "    - A. Eisenberg, S. Gannot and S. E. Chazan, \"Single microphone speaker extraction using unified time-frequency Siamese-Unet,\" 2022 30th European Signal Processing Conference (EUSIPCO), Belgrade, Serbia, 2022, pp. 762-766, doi: 10.23919/EUSIPCO55093.2022.9909545 : https://arxiv.org/pdf/2203.02941.pdf\n",
    "    - K. Zmolikova, M. Delcroix, T. Ochiai, K. Kinoshita, J. Černocký and D. Yu, \"Neural Target Speech Extraction: An overview,\" in IEEE Signal Processing Magazine, vol. 40, no. 3, pp. 8-29, May 2023, doi: 10.1109/MSP.2023.3240008. keywords: {Deep learning;Visualization;Targeting;Signal processing;Reverberation;Arrays;Speech processing;Noise measurement;Oral communication},:https://arxiv.org/pdf/2301.13341.pdf \n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
